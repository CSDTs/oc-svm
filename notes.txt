2/12
See; https://hackernoon.com/one-class-classification-for-images-with-deep-features-be890c43455d
See: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html

See: https://github.com/onnx/models#image_classification
for layers, note MobileNet and ShuffleNet

for a good way to use CNNs on images as features and then pass
to the OC-SVM for training, prediction

The issue is that we don't really understand the feature space, depending on
the CNN, we have these texture basises, not sure how to manipulate or explain that

Could have to students learn the basis? Would be easier if it was better grounded
--

2/14
See Skorch for working in pytorch
https://skorch.readthedocs.io/en/stable/user/installation.html

So there's something of a slight decision point,
The first hackernoon reference shows an end to end way to train on OC SVM with CNN features,
using Keras. But I've been advised to use PyTorch. I also want to get results ASAP.
There is also a deep OC features approach, here: https://github.com/PramuPerera/DeepOneClass
it uses Caffe though.

Okay so I think I should just reproduce the tutorial above and then swap in my dataaset and
report on results, set up an API somewhere
--
2/18

Okay, so what I'll do here is reproduce https://hackernoon.com/one-class-classification-for-images-with-deep-features-be890c43455d
within this cookie cutter data science frame work

note, vscode hot tip, fn+shift  on  mac in ipython will recall the previously entered command w/o 
scrolling up in the current command
-
note that tensorflow has no support for GPUs on macs (not NVIDIA) :___(
    this means if i run this elsewhere I should make sure I'm using the GPU'ed tensorflow
    or run w/in a Google colab
--
I'm starting to think that if i want to have results in hand before 5pm I might be
better off just busting this out in ipython, save the output and port to a the cookie cutter
reproducibale format later.
--
k, let's try it ...

okay I get
(oc-svm) bash-3.2$ python extract_features.py 
Using TensorFlow backend.
[INFO] loading network...
2020-02-18 15:14:22.733788: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-18 15:14:22.796471: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc9ebc3c6c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-18 15:14:22.796496: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
/Users/kwamepr/.local/share/virtualenvs/oc-svm-EfKF6l7S/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.
  warnings.warn('The output shape of `ResNet50(include_top=False)` '
Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5
94658560/94653016 [==============================] - 4s 0us/step

but note
```
The output of the max-pooling layer has a volume shape of 7 x 7 x 512 which we flatten into a feature vector of 21,055-dim.
```
which suggests that I should do pool=max
see:
https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py#L256-L266
pool = 'max'

we should see a 7 x 7 512 matrix, flatted into a 21,055 dim feature vector
(for one image!?)
--
anyway, let's re-run with this 
works
-
say what now???

(oc-svm) bash-3.2$ ls -l output/validation.csv 
-rw-r--r--  1 kwamepr  staff  0 Feb 18 15:26 output/validation.csv
--
(oc-svm) bash-3.2$ ls -l output/*
-rw-r--r--  1 kwamepr  staff    0 Feb 18 15:26 output/evaluation.csv
-rw-r--r--  1 kwamepr  staff  261 Feb 18 15:26 output/le.cpickle
-rw-r--r--  1 kwamepr  staff    0 Feb 18 15:26 output/training.csv
-rw-r--r--  1 kwamepr  staff    0 Feb 18 15:26 output/validation.csv
---
note: https://cv-tricks.com/keras/understand-implement-resnets/
see the data generater shear, zoom , flip tricks

okay, so from the above there a couple of things to try:
a) Theyuse an avg pool, the write up from the code says a max pool but
maybe that was a typo?
b) Use no pool and see what happens, the pooling will change the shape.

I'll start w b since both writeups don't specify a pooling. Note this will
trigger the warning
-
I'll be, it's running
---
great got the following in the 5k replication

93/93 [==============================] - 120s 1s/step - loss: 2.9558e-05 - accuracy: 1.0000 - val_loss: 1.9939e-05 - val_accuracy: 0.9876
[INFO] evaluating network...
              precision    recall  f1-score   support

        food       0.99      0.98      0.99       500
    non_food       0.98      0.99      0.99       500

    accuracy                           0.99      1000
   macro avg       0.99      0.99      0.99      1000
weighted avg       0.99      0.99      0.99      1000
---
notice that classification metric is the same but the last epoch isn't as good
---
Now I need to
a) Figure out how to move training/test examples 
b) Figure out how to partially train a one class svm (I think?)
----
Okay, for SGD, I got

Epoch 3/3
78/78 [==============================] - 107s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 3.8343 - val_accuracy: 0.7232
[INFO] evaluating network...
/Users/kwamepr/.local/share/virtualenvs/oc-svm-EfKF6l7S/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

        fake       0.00      0.00      0.00       188
        real       0.73      1.00      0.84       500

    accuracy                           0.73       688
   macro avg       0.36      0.50      0.42       688
weighted avg       0.53      0.73      0.61       688
---
Which is really bad it seems, although it must be predicting some things are fake because 
it gets .73 on the real images, so I'm not sure. But it thinks all fake
images are real which ... isn't good.
---

Anyway, I'm trying to load any of the data and it's taking a looooooong time.
Eval is       327 mb 
training is   1g
validation is 325 mb

it looks like csv_feature_generator is doign something slightly unique,
maybe I should just workw with that instead of pandas; the nueral network 
does start training easily so it does load the data fine.

also, the generator will re-read data to fill up to the batch size,
wihch is really bad
---

see; https://stackoverflow.com/questions/40100176/can-dask-parralelize-reading-fom-a-csv-file
for loading dataframes in parallel with Dask,
 the followign works pretty quickly (~ 1 min for training data)
```
from dask.distributed import progress
from dask.distributed import Client
from dask import dd
client = Client()

df_dask = dd.read_csv(trainPath, sep=',', dtype=float, sample=int(2*256e3), blocksize="512MB")
df = progress(df_dask.compute(scheduler='processes'))
```

Rosebrock has a tutorial on deep learning anomaly detection 
here: https://www.pyimagesearch.com/2020/03/02/anomaly-detection-with-keras-tensorflow-and-deep-learning/
that uses autoencoder, autoencoder error as an anomaly dectector.

The approach seems a bit messy, it assumes a known proportion of other class
which kind of breaks the assumption of one class and I really wonder if reconstructoin error, as
opposed to dimensional space is the right way to go about the problem
-
Okay, loading into an numpy array directly was wayyyyyyy easier
defintely a score for KISS. It's tough though because sustainability requires
good code design and eliminating code debt but research coding isn't really
meant to go outside of the basic proof of concept :(

I think I'm good, going to reorder the paths and start w the training data first
then run. From here I should be able to push the entire array, seperate out the labels,
into a one class classifier after pca'ing
--
okay, so I'm getting ~~slightly better~~ exactly the same results from one class svm,

ted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

        fake       0.00      0.00      0.00       187
        real       0.73      1.00      0.84       500

    accuracy                           0.73       687
   macro avg       0.36      0.50      0.42       687
weighted avg       0.53      0.73      0.61       687

Isolation Forest ...
Traceback (most recent call last):
  File "train.py", line 193, in <module>


The isolation forest appears to be predicting 3 classes instead of 2 which is
very odd and throws an error.  I put in code to count the number of
unique predictiosn to veriy that we're not seeing all real (1) predictions,
which should be the case since the real line has a precision of 0.73.
I suspect, somehow, what is happening that any fake predictions are only
happening when presented a real example. I should probably get an AUC score.

I will run this in ipython, i want to debug the isolationforest but that's not
really pressing now, hmm well , yeah I need to figure out the distirbution of
predictions and then decide
---
I think the space of real and fake kente might be concentric, with fake being
within real, for the image net feature set

I think I need more texture and invariant luminance checks. Let me see if there is
a auto-image feature generator
---
[INFO] class prediction counts {np.unique(oc_svm_preds, return_counts=True)}
/Users/kwamepr/.local/share/virtualenvs/oc-svm-EfKF6l7S/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

        fake       0.00      0.00      0.00       187
        real       0.73      1.00      0.84       500

    accuracy                           0.73       687
   macro avg       0.36      0.50      0.42       687
weighted avg       0.53      0.73      0.61       687

0.5
Isolation Forest ...
[INFO] class prediction counts {np.unique(if_preds, return_counts=True)}
/Users/kwamepr/.local/share/virtualenvs/oc-svm-EfKF6l7S/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

        fake       0.00      0.00      0.00       187
        real       0.73      1.00      0.84       500

    accuracy                           0.73       687
   macro avg       0.36      0.50      0.42       687
weighted avg       0.53      0.73      0.61       687

0.5
[INFO] done!

occured when running w/in ipython, not sure why I got a different result since
nothign was changed.
---
see: https://gogul.dev/software/texture-recognition#what-is-a-texture
for harlick feature, wonder if it runs over RGB space instead of just gray scale
--
ah interseting
I just observed that the training folder has instances of fake kente cloth in it
this means that those instances are folded into its notion of real kente, wihch is
very much incorrect and could explain why all fake kente is preceived as real since
the variation among our fake kente is smaller.

Another thing I noticed or though about is that the grayscale space that imagenet and
others operate in naturally removes color intensity, which is an important component of
artisan work, perhaps evenmore so than Western art (this probably could be measured somehow)

This recalls how original video cameras didn't provifde enough contrast for darker skinned folks,
see: https://www.buzzfeednews.com/article/syreetamcfadden/teaching-the-camera-to-see-my-skin

There is defintely a high tech analogy to emulsion range with the standardization of, say,
image net, using grayscale, for image classification, where it performs less on 
high contrast, dynamic imagery (including some kinds of art).
Note: there is bias in imagenet, this is beign worked on
---
Okay, so we're getting a ROC AUC of 0.5 which is random guessing :( something
doesn't quite add up because some of the real where predicted to be fake, so it can
output a fake decision, just not very smartly.
-------

Okay, so, I really want to get this out of the way so I can enjoy my spring break,
I need to get better accuracy on both cases.

Doing a problem break down on this, i suspect

see: https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/

for a nice write up of color spaces
--
see vtreat, application to unsupervised problems: https://github.com/WinVector/pyvtreat/blob/master/Examples/Unsupervised/Unsupervised.md
--
See this write up on whiteneing, http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/
it's defintely for images though, but in this code base it's applied to
imagenet likelihoods, which seems somewhat strange to me.
----
Okay, correcting the class labels fixed the isolation forest error, etc, but the AUC is the same

[INFO] producing classification report ...
One Class SVM...
[INFO] class prediction counts (array([-1,  1]), array([  1, 686]))
              precision    recall  f1-score   support

        fake       0.00      0.00      0.00       187
        real       0.73      1.00      0.84       500

    accuracy                           0.73       687
   macro avg       0.36      0.50      0.42       687
weighted avg       0.53      0.73      0.61       687

ROC AUC Score:  0.499
Isolation Forest ...
[INFO] class prediction counts (array([-1,  1]), array([  1, 686]))
              precision    recall  f1-score   support

        fake       0.00      0.00      0.00       187
        real       0.73      1.00      0.84       500

    accuracy                           0.73       687
   macro avg       0.36      0.50      0.42       687
weighted avg       0.53      0.73      0.61       687

ROC AUC Score:  0.499
----

okay, so I got it to output hsv features only (it's a lot faster to do so)
now i need to train and predict against. I'm also pretty much on my 
last problem breakdown approach.


okay ....
given
```
bins = np.linspace(30, 60, 100)
my_x = oc_svm_clf.score_samples(X_val)
plt.hist(my_x[y_val==-1], bins, alpha=0.5, label='-1')
plt.hist(my_x[y_val==1], bins, alpha=0.5, label='1')
plt.legend(loc='upper right')
```

okay, looking at the historgram we defintely see some kind of seperation
it's disjoint across the score_samples domain but it's there.
I'm re-running with image net features to see if it's any better seperated
-
it's basically the same with a lot more overhead

ROC AUC Score:  0.5
Isolation Forest ...
[INFO] class prediction counts (array([1]), array([687]))
/Users/kwamepr/.local/share/virtualenvs/oc-svm-EfKF6l7S/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
              precision    recall  f1-score   support

        fake       0.00      0.00      0.00       187
        real       0.73      1.00      0.84       500

    accuracy                           0.73       687
   macro avg       0.36      0.50      0.42       687
weighted avg       0.53      0.73      0.61       687

ROC AUC Score:  0.5
[INFO] done!
---

So it's pretty clear that the images and/or feature space is at issue.

I do think there is something subtle about the self consistency, tiled repeated shapes,
fractal dimension of. 

see: https://www.codementor.io/@mathiasgatti/the-beauty-formula-identifying-interesting-patterns-automatically-based-on-aesthetic-metrics-basic-clustering-example-with-scikit-learn-xka5d6do8
--
okay imagenet is either the same or slightly worse; the left tail isn't as good.

--
okay so this is what I think I should do:
a) consider applying fractical dimension with an entropy measure (don't need to divide gif/tiff size)
b) threshold the svm output (or something) to get a better AUC, F1 score 
or
c) use a one class sgd? but throw in one fake instance so it can handle fake isntances as a class
fill in paper with the above.

I do recognize from this that this representation presents a challenging problem
and that better features are probably needed.
----
3/4

okay, I don't really want to drop more time into this, esp this week, beyond
today, so what i'll do is update my problem 
--
back, okay, took the morning to analyze the problem and I came up with 3 or 4 things
to consider. Aside from that, also, I broke this down into a rapid iteration pipeline
which i should've done to being with but I assumed the classifier was more robust than it was.
-
So what I'm going to do is
a) From https://sdsawtelle.github.io/blog/output/week9-anomaly-andrew-ng-machine-learning-with-python.html, 
set up a grid search framework that uses subset of training data (positive only)
b) The grid search will use the F1 score, as recommended, against a subset of the validation data set, which
includes negative cases.
c) I will also use their 3d visualization, which is just nice to look at.

under small subsets this should be a fairly rapid search pipeline, from there
i can throw differetn feature techniques (SSIM, isolated woven structures, etc), I'll start this
if the pipeline isn't more helpful.
------
okay so I'm running the grid search and it's pretty slow bu tI am assing
F1 scores of .915. I'm not sure what the other components of the score are,
like false positive, etc.

Givcen the time the search is taking, this is not a rapid iteration pipeline, though.
----

Okay, i've been in the trenches and I've been out of mind, I think I'm ready to call it here:

              precision    recall  f1-score   support

        -1.0       0.13      0.54      0.20        26
         1.0       0.97      0.80      0.88       479

    accuracy                           0.78       505
   macro avg       0.55      0.67      0.54       505
weighted avg       0.93      0.78      0.84       505


In [32]: optimal_forest                                                                                                                              
Out[32]: 
LocalOutlierFactor(algorithm='auto', contamination='auto', leaf_size=30,
                   metric='correlation', metric_params=None, n_jobs=-1,
                   n_neighbors=3, novelty=True, p=2)
---
The only other thing I can think of, beside figuring how to implement
a floating weave symbol detector, is to change the scorer
---
[Parallel(n_jobs=-1)]: Done 414 out of 414 | elapsed:   22.1s finished
              precision    recall  f1-score   support

        -1.0       0.13      0.54      0.20        26
         1.0       0.97      0.80      0.88       479

    accuracy                           0.78       505
   macro avg       0.55      0.67      0.54       505
weighted avg       0.93      0.78      0.84       505


using balanced_accuracy
----
3/5
okay, these are some possible review items to add in if there's time/interest:
ADD INs
* Use ROC AUC score with a limit on the FPR such that the negative class is
bounded to, say, 70% accuracy
* Consider a dimensionality reduction that is class focused (LDA, Neighborhood component
analysis, NCA)
---
Okay, I read the write up (really good I did), here's what I need to do:

X * We're switching back to the ImageNet CNN. Uh, I use the Food 5k encoder but I should use, say, 
AlexNet or somethign that was used in texture (AlexNet was)
  X Use ResNet 50 model; we could use MobileNet but I want to move on to the next steps here.
  X NOTE:  I'm starting to think it's better to figure out a jypter notebook setup for
  reproducibilty here. Doing it as a cookiecutter data science project is great for
  applications, things with a wider reach, but navigation across multiple files is pretty annoying.
    X yeah my next project will be done in a Juypter notebook, so much easier
      X quick break, need to send out some emails

* I need to then do dimensinality reduction with NCA since we're using a LocalOUtlier thingie
  X turnsout the twoclass.npy has a different set of dimension :(, oh i think it's .hsv? hard to say,
  X Need to re-run everything, might as well use mobile net
    X k let's try this, re-run

--
     ...: ^Inca.fit(X_train, y_train) 
     ...:                                                                                                                                            
Finding principal components... done in 52.07s
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =     32112640     M =           10
 This problem is unconstrained.

At X0         0 variables are exactly at the bounds
[NeighborhoodComponentsAnalysis]
[NeighborhoodComponentsAnalysis]  Iteration      Objective Value    Time(s)
[NeighborhoodComponentsAnalysis] ------------------------------------------
[NeighborhoodComponentsAnalysis]          1         2.698000e+03      11.90

At iterate    0    f= -2.69800D+03    |proj g|=  1.37618D-06

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
*****      0      1      0     0     0   1.376D-06  -2.698D+03
  F =  -2698.0000000005057     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
[NeighborhoodComponentsAnalysis] Training took    94.05s.
--

**I need to make a scree plot and pick the "knee", there should be one since images themselves
are highly redunant
I can also plot up to the first three dimension
  First three dimensions show concetration 
  ** So I think it's worth trying a collection of component analyses and
  taking the best one in the validation data set.


* Then I need to run my hyper parameter optizaiont, peep results,
graph best on held out validation sample.

* Then I need to run on test data, probably plot, generate results
PR curve, ROC, etc.
----
In [21]:     pca1 = PCA(n_components=int(X_train.shape[0]*0.50), whiten=True).fit(X_train) 
            print('Explained variance percentage = %0.2f' % sum(pca1.explained_variance_ratio_)) 
Explained variance percentage = 0.85

In [11]:     pca2 = PCA(n_components=int(X_train.shape[0]*0.25), whiten=True).fit(X_train)                                                           
In [12]:     print('Explained variance percentage = %0.2f' % sum(pca2.explained_variance_ratio_))                                                    
Explained variance percentage = 0.68

In [13]:     pca3 = PCA(n_components=int(X_train.shape[0]*0.125), whiten=True).fit(X_train)                                                          
In [14]:     print('Explained variance percentage = %0.2f' % sum(pca3.explained_variance_ratio_))                                                    
Explained variance percentage = 0.53

In [15]:     pca4 = PCA(n_components=int(X_train.shape[0]*0.05), whiten=True).fit(X_train)                                                           
In [16]:     print('Explained variance percentage = %0.2f' % sum(pca4.explained_variance_ratio_))                                                    
Explained variance percentage = 0.38

In [21]: [int(X_train.shape[0]*prop) for prop in [0.5, 0.25, 0.125, 0.05]]                                                                           
Out[21]: [1365, 682, 341, 136]  
^ dimensions
----
So I think what i'll do is run the above with 